Using TensorFlow backend.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
Using TensorFlow backend.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
Starting with session 0
test case : 1
#################################################################################
path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
fixed_path
 [[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]
train_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
infer_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
Number of layers being trained :  13

Epoch: [1 | 10] LR: 0.001000 Sess: 0

Epoch: [2 | 10] LR: 0.001000 Sess: 0

Epoch: [3 | 10] LR: 0.001000 Sess: 0

Epoch: [4 | 10] LR: 0.001000 Sess: 0

Epoch: [5 | 10] LR: 0.001000 Sess: 0

Epoch: [6 | 10] LR: 0.001000 Sess: 0

Epoch: [7 | 10] LR: 0.000500 Sess: 0

Epoch: [8 | 10] LR: 0.000500 Sess: 0

Epoch: [9 | 10] LR: 0.000250 Sess: 0

Epoch: [10 | 10] LR: 0.000250 Sess: 0
Best acc:
98.86461937716263
Labels 1: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
Labels 2: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
torch.Size([1, 3, 32, 32])
Truth: 0
torch.Size([1, 3, 32, 32])
Truth: 1
session_0_0_log.txt
session_0_1_log.txt
98.864619 1
tensor([[4966.,   27.,    0.,  ...,    0.,    0.,    0.],
        [  28., 4037.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        ...,
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.]])
done with session 0
#################################################################################
5
5
5
5
5
5
5
5
5
6
6
6
6
6
6
6
6
6
6
6
6
7
7
7
7
7
7
7
7
7
7
7
7
7
8
8
8
8
8
8
8
8
8
8
8
8
9
9
9
9
9
9
9
9
9
9
9
9
9
10
10
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
Starting with session 0
test case : 0
#################################################################################
path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
fixed_path
 [[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]
train_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
infer_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
Number of layers being trained :  13

Epoch: [1 | 10] LR: 0.001000 Sess: 0

Epoch: [2 | 10] LR: 0.001000 Sess: 0

Epoch: [3 | 10] LR: 0.001000 Sess: 0

Epoch: [4 | 10] LR: 0.001000 Sess: 0

Epoch: [5 | 10] LR: 0.001000 Sess: 0

Epoch: [6 | 10] LR: 0.001000 Sess: 0

Epoch: [7 | 10] LR: 0.000500 Sess: 0

Epoch: [8 | 10] LR: 0.000500 Sess: 0

Epoch: [9 | 10] LR: 0.000250 Sess: 0

Epoch: [10 | 10] LR: 0.000250 Sess: 0
Best acc:
98.43209342560553
Labels 1: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
Labels 2: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
torch.Size([1, 3, 32, 32])
Truth: 0
torch.Size([1, 3, 32, 32])
Truth: 1
session_0_0_log.txt
session_0_1_log.txt
98.864619 1
tensor([[4.8240e+03, 6.6000e+01, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.9000e+01, 3.9930e+03, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]])
done with session 0
#################################################################################
10
10
Using TensorFlow backend.
/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
Using TensorFlow backend.
/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_0_0_log.txt
session_0_1_log.txt
98.864619 1
Starting with session 1
test case : 0
#################################################################################
path
 [[0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]]
fixed_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
train_path
 [[0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]]
infer_path
 [[1. 0. 0. 0. 0. 0. 0. 1.]
 [1. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0.]]
Number of layers being trained :  13

Epoch: [1 | 10] LR: 0.001000 Sess: 1

Epoch: [2 | 10] LR: 0.001000 Sess: 1

Epoch: [3 | 10] LR: 0.001000 Sess: 1

Epoch: [4 | 10] LR: 0.001000 Sess: 1

Epoch: [5 | 10] LR: 0.001000 Sess: 1

Epoch: [6 | 10] LR: 0.001000 Sess: 1

Epoch: [7 | 10] LR: 0.000500 Sess: 1

Epoch: [8 | 10] LR: 0.000500 Sess: 1

Epoch: [9 | 10] LR: 0.000250 Sess: 1

Epoch: [10 | 10] LR: 0.000250 Sess: 1
Best acc:
96.02129257224017
Labels 1: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
Labels 2: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
torch.Size([1, 3, 32, 32])
Truth: 0
torch.Size([1, 3, 32, 32])
Truth: 1
session_1_0_log.txt
session_1_1_log.txt
96.041766 1
tensor([[4670.,   18.,  188.,  ...,    0.,    0.,    0.],
        [  25., 3844.,  224.,  ...,    0.,    0.,    0.],
        [   6.,    8., 2849.,  ...,    0.,    0.,    0.],
        ...,
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.]])
done with session 1
#################################################################################
10
9
10
9
10
10
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_0_0_log.txt
session_0_1_log.txt
98.864619 1
Starting with session 1
test case : 1
#################################################################################
path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]]
fixed_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]]
train_path
 [[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]]
infer_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]]
Number of layers being trained :  12

Epoch: [1 | 10] LR: 0.001000 Sess: 1

Epoch: [2 | 10] LR: 0.001000 Sess: 1

Epoch: [3 | 10] LR: 0.001000 Sess: 1

Epoch: [4 | 10] LR: 0.001000 Sess: 1

Epoch: [5 | 10] LR: 0.001000 Sess: 1

Epoch: [6 | 10] LR: 0.001000 Sess: 1

Epoch: [7 | 10] LR: 0.000500 Sess: 1

Epoch: [8 | 10] LR: 0.000500 Sess: 1

Epoch: [9 | 10] LR: 0.000250 Sess: 1

Epoch: [10 | 10] LR: 0.000250 Sess: 1
Best acc:
96.04176619538902
Labels 1: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
Labels 2: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
torch.Size([1, 3, 32, 32])
Truth: 0
torch.Size([1, 3, 32, 32])
Truth: 1
session_1_0_log.txt
session_1_1_log.txt
96.041766 1
tensor([[4734.,   23.,  154.,  ...,    0.,    0.,    0.],
        [  22., 3870.,  192.,  ...,    0.,    0.,    0.],
        [  12.,   18., 2839.,  ...,    0.,    0.,    0.],
        ...,
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.]])
done with session 1
#################################################################################
10
10
Using TensorFlow backend.
/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
Using TensorFlow backend.
/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_1_0_log.txt
session_1_1_log.txt
96.041766 1
Starting with session 2
test case : 1
#################################################################################
path
 [[0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0.]]
fixed_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]]
train_path
 [[0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0.]]
infer_path
 [[1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 1. 0. 0. 1.]
 [1. 1. 0. 0. 0. 1. 0. 0.]]
Number of layers being trained :  11

Epoch: [1 | 10] LR: 0.001000 Sess: 2

Epoch: [2 | 10] LR: 0.001000 Sess: 2

Epoch: [3 | 10] LR: 0.001000 Sess: 2

Epoch: [4 | 10] LR: 0.001000 Sess: 2

Epoch: [5 | 10] LR: 0.001000 Sess: 2

Epoch: [6 | 10] LR: 0.001000 Sess: 2

Epoch: [7 | 10] LR: 0.000500 Sess: 2

Epoch: [8 | 10] LR: 0.000500 Sess: 2

Epoch: [9 | 10] LR: 0.000250 Sess: 2

Epoch: [10 | 10] LR: 0.000250 Sess: 2
Best acc:
93.84663932240949
Labels 1: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
Labels 2: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
torch.Size([1, 3, 32, 32])
Truth: 0
torch.Size([1, 3, 32, 32])
Truth: 1
session_2_1_log.txt
session_2_0_log.txt
93.846639 1
tensor([[4723.,   37.,   30.,  ...,    0.,    0.,    0.],
        [  25., 3862.,   65.,  ...,    0.,    0.,    0.],
        [  19.,   13., 2478.,  ...,    0.,    0.,    0.],
        ...,
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.]])
done with session 2
#################################################################################
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
7
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
8
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
9
10
10
Using TensorFlow backend.
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_2_1_log.txt
session_2_0_log.txt
93.846639 1
Starting with session 3
test case : 0
#################################################################################
path
 [[0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0.]]
fixed_path
 [[1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 1. 0. 0. 1.]
 [1. 1. 0. 0. 0. 1. 0. 0.]]
train_path
 [[0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]]
infer_path
 [[1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 1. 0. 1.]
 [1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 1. 0. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0.]
 [1. 0. 0. 0. 1. 0. 0. 1.]
 [1. 1. 0. 0. 0. 1. 1. 0.]]
Number of layers being trained :  11

Epoch: [1 | 10] LR: 0.001000 Sess: 3
Traceback (most recent call last):
  File "svhn.py", line 372, in <module>
    main()
  File "svhn.py", line 340, in main
    pred = main_learner.learn()
  File "/home/abanyi17/RPSnet-master/learner.py", line 97, in learn
    self.train(epoch, self.infer_path, -1)
  File "/home/abanyi17/RPSnet-master/learner.py", line 173, in train
    outputs_old=self.old_model(inputs, path, -1)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abanyi17/RPSnet-master/rps_net.py", line 447, in forward
    y = y + self.conv3[j](x)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 179, in forward
    self.eps,
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/functional.py", line 2283, in batch_norm
    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.17 GiB total capacity; 5.20 GiB already allocated; 25.50 MiB free; 5.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_1_0_log.txt
session_1_1_log.txt
96.041766 1
Starting with session 2
test case : 0
#################################################################################
path
 [[0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]]
fixed_path
 [[1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0.]]
train_path
 [[0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]]
infer_path
 [[1. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 1. 0. 0. 0. 1.]
 [1. 1. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 1.]
 [1. 1. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0.]]
Number of layers being trained :  12

Epoch: [1 | 10] LR: 0.001000 Sess: 2

Epoch: [2 | 10] LR: 0.001000 Sess: 2

Epoch: [3 | 10] LR: 0.001000 Sess: 2

Epoch: [4 | 10] LR: 0.001000 Sess: 2

Epoch: [5 | 10] LR: 0.001000 Sess: 2

Epoch: [6 | 10] LR: 0.001000 Sess: 2

Epoch: [7 | 10] LR: 0.000500 Sess: 2

Epoch: [8 | 10] LR: 0.000500 Sess: 2

Epoch: [9 | 10] LR: 0.000250 Sess: 2

Epoch: [10 | 10] LR: 0.000250 Sess: 2
Best acc:
93.39960030254485
Labels 1: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
Labels 2: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
torch.Size([1, 3, 32, 32])
Truth: 0
torch.Size([1, 3, 32, 32])
Truth: 1
session_2_1_log.txt
session_2_0_log.txt
93.846639 1
tensor([[4681.,   37.,   38.,  ...,    0.,    0.,    0.],
        [  29., 3861.,   78.,  ...,    0.,    0.,    0.],
        [  22.,   14., 2523.,  ...,    0.,    0.,    0.],
        ...,
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],
        [   0.,    0.,    0.,  ...,    0.,    0.,    0.]])
done with session 2
#################################################################################
10
10
Using TensorFlow backend.
/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_2_1_log.txt
session_2_0_log.txt
93.846639 1
Starting with session 3
test case : 1
#################################################################################
path
 [[0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]]
fixed_path
 [[1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0.]
 [1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 1. 0. 0. 1.]
 [1. 1. 0. 0. 0. 1. 0. 0.]]
train_path
 [[0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]]
infer_path
 [[1. 1. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 1. 1.]
 [1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 1. 0.]
 [1. 1. 0. 1. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 1. 1. 0.]
 [1. 1. 0. 0. 1. 0. 0. 1.]
 [1. 1. 0. 0. 1. 1. 0. 0.]]
Number of layers being trained :  11

Epoch: [1 | 10] LR: 0.001000 Sess: 3
Traceback (most recent call last):
  File "svhn.py", line 372, in <module>
    main()
  File "svhn.py", line 340, in main
    pred = main_learner.learn()
  File "/home/abanyi17/RPSnet-master/learner.py", line 98, in learn
    pred = self.test(epoch, self.infer_path, -1)
  File "/home/abanyi17/RPSnet-master/learner.py", line 257, in test
    outputs = self.model(inputs, path, -1)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abanyi17/RPSnet-master/rps_net.py", line 458, in forward
    y = self.conv5[0](x)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/modules/activation.py", line 98, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/torch/nn/functional.py", line 1299, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 11.17 GiB total capacity; 10.20 GiB already allocated; 60.88 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Using TensorFlow backend.
/home/abanyi17/RPSnet-master/util.py:21: UserWarning: loadtxt: Empty input file: "results/svhn/RPS_net_svhn/session_3_0_log.txt"
  f = np.loadtxt(checkpoint+"/"+file, skiprows=1)
/home/abanyi17/RPSnet-master/util.py:21: UserWarning: loadtxt: Empty input file: "results/svhn/RPS_net_svhn/session_3_1_log.txt"
  f = np.loadtxt(checkpoint+"/"+file, skiprows=1)
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_3_0_log.txt
session_3_1_log.txt
Traceback (most recent call last):
  File "svhn.py", line 372, in <module>
    main()
  File "svhn.py", line 226, in main
    load_test_case = get_best_model(ses-1, args.checkpoint)
  File "/home/abanyi17/RPSnet-master/util.py", line 33, in get_best_model
    a = np.argmax(best_acc)
  File "<__array_function__ internals>", line 6, in argmax
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/numpy/core/fromnumeric.py", line 1188, in argmax
    return _wrapfunc(a, 'argmax', axis=axis, out=out)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/numpy/core/fromnumeric.py", line 55, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/numpy/core/fromnumeric.py", line 44, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
ValueError: attempt to get argmax of an empty sequence
Using TensorFlow backend.
/home/abanyi17/RPSnet-master/util.py:21: UserWarning: loadtxt: Empty input file: "results/svhn/RPS_net_svhn/session_3_0_log.txt"
  f = np.loadtxt(checkpoint+"/"+file, skiprows=1)
/home/abanyi17/RPSnet-master/util.py:21: UserWarning: loadtxt: Empty input file: "results/svhn/RPS_net_svhn/session_3_1_log.txt"
  f = np.loadtxt(checkpoint+"/"+file, skiprows=1)
{'epochs': 10, 'checkpoint': 'results/svhn/RPS_net_svhn', 'savepoint': 'results/svhn/pathnet_svhn', 'dataset': 'SVHN', 'num_class': 10, 'class_per_task': 2, 'M': 8, 'L': 9, 'N': 1, 'lr': 0.001, 'train_batch': 128, 'test_batch': 128, 'workers': 16, 'resume': False, 'arch': 'res-18', 'start_epoch': 0, 'evaluate': False, 'sess': 0, 'test_case': 0, 'schedule': [6, 8, 16], 'gamma': 0.5, 'rigidness_coff': 2.5, 'jump': 1}
    Total params: 89.55M
session_3_0_log.txt
session_3_1_log.txt
Traceback (most recent call last):
  File "svhn.py", line 372, in <module>
    main()
  File "svhn.py", line 226, in main
    load_test_case = get_best_model(ses-1, args.checkpoint)
  File "/home/abanyi17/RPSnet-master/util.py", line 33, in get_best_model
    a = np.argmax(best_acc)
  File "<__array_function__ internals>", line 6, in argmax
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/numpy/core/fromnumeric.py", line 1188, in argmax
    return _wrapfunc(a, 'argmax', axis=axis, out=out)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/numpy/core/fromnumeric.py", line 55, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File "/home/abanyi17/RPSnet-master/captum/lib64/python3.6/site-packages/numpy/core/fromnumeric.py", line 44, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
ValueError: attempt to get argmax of an empty sequence
